{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5bc3c4fa-fbe9-4c99-b6b6-88e159b4fd69",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Step 1: Create Catalog Structure (SQL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6a7c21ff-ced6-450f-9345-96b693e38141",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SHOW CATALOGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "52f6b733-e030-4547-a157-0f2d34b426ff",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- Create retailplex_platform catalog\n",
    "CREATE CATALOG IF NOT EXISTS retailplex_platform;\n",
    "USE CATALOG retailplex_platform;\n",
    "\n",
    "-- Create schemas for medallion layers\n",
    "CREATE SCHEMA IF NOT EXISTS retailplex_platform.landing;\n",
    "CREATE SCHEMA IF NOT EXISTS retailplex_platform.bronze;\n",
    "\n",
    "CREATE SCHEMA IF NOT EXISTS retailplex_platform.gold;\n",
    "\n",
    "-- Create volume for files\n",
    "CREATE VOLUME IF NOT EXISTS retailplex_platform.landing.raw_files;\n",
    "\n",
    "\n",
    "-- Create volume for schema and checkpoints\n",
    "CREATE VOLUME IF NOT EXISTS retailplex_platform.bronze.schemas\n",
    "COMMENT 'Volume for bronze schemas';\n",
    "CREATE VOLUME IF NOT EXISTS retailplex_platform.bronze.checkpoints\n",
    "COMMENT 'Volume for bronze checkpoints';\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a0477d3f-d199-4728-bfef-1a6d66cd9f12",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Step 2: Upload Your JSON File\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "18ee23f9-e403-4904-804e-3b26c65c7232",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# the methods will not work. better to upload directly here /Volumes/retailplex_platform/landing/raw_files/incoming_multiplex_data/multiplex_data.jsonl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "661d4e43-1fa1-4292-8d06-041711c01ac4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Step 3: Process the stream and Save Bronze layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "33cac8de-9fc3-41d1-a7a8-184bad791cea",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f86904c6-b827-4d69-8f48-6a8aa1942424",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Set Spark configs before any Spark actions\n",
    "spark.conf.set(\"spark.databricks.delta.optimizeWrite.enabled\", \"true\")\n",
    "spark.conf.set(\"spark.databricks.delta.autoCompact.enabled\", \"true\")\n",
    "spark.conf.set(\"spark.sql.shuffle.partitions\", \"200\")\n",
    "spark.conf.set(\"spark.sql.adaptive.enabled\", \"true\")\n",
    "spark.conf.set(\"spark.sql.adaptive.coalescePartitions.enabled\", \"true\")\n",
    "spark.conf.set(\"spark.sql.adaptive.skewJoin.enabled\", \"true\")\n",
    "spark.conf.set(\"spark.sql.streaming.schemaInference\", \"true\")\n",
    "spark.conf.set(\"spark.sql.broadcastTimeout\", \"1200\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fd805c31-e605-41db-bfab-edd5a2f57e67",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(\"🌊 Setting up Auto Loader for Streaming Source...\")\n",
    "\n",
    "# Auto Loader paths\n",
    "streaming_source_path = \"/Volumes/retailplex_platform/landing/raw_files/incoming_multiplex_data/\"\n",
    "schema_location = \"/Volumes/retailplex_platform/bronze/schemas/retailplex_stream/\"\n",
    "checkpoint_location =  \"/Volumes/retailplex_platform/bronze/checkpoints/retailplex_stream/\"\n",
    "# 2. Define Bronze Delta table path (managed by Unity Catalog schema bronze)\n",
    "bronze_table = \"retailplex_platform.bronze.multiplex_stream\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "98ed3cb4-6085-4807-b0da-f10b398d25c4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "multiplex_schema = StructType([\n",
    "    StructField(\"topic\", StringType(), True),\n",
    "    StructField(\"timestamp\", StringType(), True),\n",
    "    StructField(\"data\", StringType(), True)\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3f8c392b-af84-454c-aa71-6ac9f38df4d7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def create_autoloader_stream():\n",
    "    \"\"\"\n",
    "    Main function to read stream and write to Bronze layer\n",
    "    \"\"\"\n",
    "    multiplex_stream = (\n",
    "        spark.readStream\n",
    "        .format(\"cloudFiles\")\n",
    "        .option(\"cloudFiles.format\", \"json\")\n",
    "        .option(\"cloudFiles.schemaLocation\", schema_location)\n",
    "        .option(\"cloudFiles.inferColumnTypes\", \"true\")\n",
    "        .option(\"cloudFiles.maxFilesPerTrigger\", 10)\n",
    "        .option(\"cloudFiles.includeExistingFiles\", \"true\")\n",
    "        .option(\"cloudFiles.validateOptions\", \"true\")\n",
    "        .schema(multiplex_schema)\n",
    "        .load(streaming_source_path)\n",
    "    )\n",
    "    # Now you can add columns\n",
    "    multiplex_stream_enhanced = (\n",
    "        multiplex_stream\n",
    "        .withColumn(\"year_month\", F.date_format(\"timestamp\", \"yyyy-MM\"))\n",
    "        .withColumn(\"_ingestion_timestamp\", F.current_timestamp())\n",
    "    )\n",
    "    multiplex_stream_write = (multiplex_stream_enhanced.writeStream \n",
    "             .format(\"delta\") \n",
    "             .outputMode(\"append\") \n",
    "             .option(\"checkpointLocation\", checkpoint_location)\n",
    "             .partitionBy(\"topic\", \"year_month\") \n",
    "             .option(\"mergeSchema\", \"true\")  \n",
    "             .trigger(availableNow=True)  \n",
    "             .table(bronze_table))\n",
    "    multiplex_stream_write.awaitTermination()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b19e461c-8d11-4f8e-be1d-40110a7ad6e7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "create_autoloader_stream()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7a3a10e8-44e5-4341-bb0d-924060980a7a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "base_in = \"/Volumes/retailplex_platform/landing/raw_files/refdata/\"\n",
    "base_bz = \"/Volumes/retailplex_platform/bronze/refdata\"\n",
    "\n",
    "pairs = [\n",
    "  (\"customer_segments\", \"CUSTOMER_SEGMENTS.csv\"),\n",
    "  (\"product_categories\", \"PRODUCT_CATEGORIES.csv\"),\n",
    "  (\"product_subcategories\", \"PRODUCT_SUBCATEGORIES.csv\"),\n",
    "  (\"suppliers\", \"SUPPLIERS.csv\"),\n",
    "  (\"geography\", \"GEOGRAPHY.csv\")\n",
    "]\n",
    "\n",
    "spark.sql(\"USE CATALOG retailplex_platform\")\n",
    "\n",
    "for tbl, file in pairs:\n",
    "    spark.sql(f\"\"\"\n",
    "      CREATE TABLE IF NOT EXISTS bronze.{tbl}\n",
    "      USING DELTA\n",
    "    \"\"\")\n",
    "    spark.sql(f\"\"\"\n",
    "      COPY INTO bronze.{tbl}\n",
    "      FROM '{base_in}/{file}'\n",
    "      FILEFORMAT = CSV\n",
    "      FORMAT_OPTIONS ('header'='true', 'inferSchema'='true')\n",
    "      COPY_OPTIONS ('mergeSchema'='true')\n",
    "    \"\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "e2b42f78-0a2a-4e0c-afb3-2e910ffd59a2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(\"\\n🔄 AUTO LOADER WORKFLOW: \\\n",
    "    \\n1. 📁 New JSON file lands in source directory,\\\n",
    "    \\n2. 🔍 Auto Loader detects the file (within seconds),\\\n",
    "    \\n3. 📋 Checks file against known schema,\\\n",
    "    \\n4. 🔄 If new columns found → evolves schema,\\\n",
    "    \\n5. ⚡ Processes file in next micro-batch,\\\n",
    "    \\n6. ✅ Updates internal tracking (never reprocess),\\\n",
    "    \\n7. 🎯 Sends data to your streaming processing function,\\\n",
    "    \\n8. 🔁 Waits for next trigger (60 seconds in our setup)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "d6ff485d-432a-45e7-a58a-dc48279de924",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# The basic Auto Loader setup\n",
    "print(\"\\n1️⃣ BASIC SETUP:\\\n",
    "\\n   multiplex_autoloader_stream = spark.readStream \\\n",
    "\\n   This creates a streaming DataFrame (not a regular DataFrame)\\\n",
    "\\n   - readStream = streaming mode (vs read for batch)\\\n",
    "\\n   - Continuously monitors for new files\")\n",
    "\n",
    "\n",
    "print(\"\\n2️⃣ FORMAT SPECIFICATION: \\\n",
    "\\n   .format('cloudFiles')\\\n",
    "\\n   - 'cloudFiles' is the Auto Loader format \\\n",
    "\\n   - This tells Spark to use Auto Loader instead of regular file reading \\\n",
    "\\n   - Auto Loader = Databricks' optimized streaming file reader\")\n",
    "\n",
    "\n",
    "print(\"\\n3️⃣ FILE FORMAT: \\\n",
    "\\n   .option('cloudFiles.format', 'json')  \\\n",
    "\\n   - Tells Auto Loader the files are JSON format \\\n",
    "\\n   - Other options: parquet, csv, text, delta, etc. \\\n",
    "\\n   - Auto Loader will parse JSON automatically\")\n",
    "\n",
    "\n",
    "print(\"\\n4️⃣ SCHEMA LOCATION: \\\n",
    "\\n   ('.option('cloudFiles.schemaLocation', schema_location)  \\\n",
    "\\n   - WHERE: /Volumes/datastream_nexus/landing/schemas/autoloader/ \\\n",
    "\\n   - WHAT: Auto Loader stores the inferred schema here \\\n",
    "\\n   - WHY: Enables schema evolution and consistency across runs \\\n",
    "\\n   - BENEFIT: If job restarts, it remembers the schema\")\n",
    "\n",
    "print(\"\\n   📁 What gets stored in schema location: \\\n",
    "\\n   ├── _schemas/ \\\n",
    "\\n   │   └── schema_version_1.json  # Discovered schema\\\n",
    "\\n   ├── _checkpoint/\\\n",
    "\\n   │   └── schema evolution tracking\\\n",
    "\\n   └── metadata files\")\n",
    "\n",
    "\n",
    "print(\"\\n5️⃣ COLUMN TYPE INFERENCE: \\\n",
    "\\n   .option('cloudFiles.inferColumnTypes', 'true') \\\n",
    "\\n   - Auto-detects data types (string, int, double, boolean) \\\n",
    "\\n   - Without this: everything would be treated as strings \\\n",
    "\\n   - Example: '123' becomes integer 123, not string '123'\")\n",
    "\n",
    "\n",
    "print(\"\\n6️⃣ SCHEMA EVOLUTION: \\\n",
    "\\n   .option(cloudFiles.schemaEvolutionMode, addNewColumns) \\\n",
    "\\n   - addNewColumns: New fields in JSON get added to schema \\\n",
    "\\n   - rescue: Puts unknown columns in _rescued_data column \\\n",
    "\\n   - failOnNewColumns: Fails if new columns appear \\\n",
    "\\n   - none: No schema evolution allowed\")\n",
    "\n",
    "\n",
    "print(\"\\n7️⃣ PROCESSING RATE CONTROL:\\\n",
    "\\n   .option('cloudFiles.maxFilesPerTrigger', 10) ')\\\n",
    "\\n   - Processes maximum 10 files per micro-batch\\\n",
    "\\n   - Prevents overwhelming the system with too many files\\\n",
    "\\n   - Helps with consistent processing latency\\\n",
    "\\n   - Tune based on file size and processing capacity\")\n",
    "\n",
    "\n",
    "print(\"\\n   📊 File Processing Strategy:\\\n",
    "\\n   Scenario 1: 5 files arrive → Process all 5\\\n",
    "\\n   Scenario 2: 50 files arrive → Process 10, queue the rest\\\n",
    "\\n   Scenario 3: No files → Wait for next trigger\")\n",
    "\n",
    "print(\"\\n8️⃣ EXISTING FILES:\\\n",
    "\\n   .option('cloudFiles.includeExistingFiles', 'true') ')\\\n",
    "\\n   - true: Process files that already exist (backfill)\\\n",
    "\\n   - false: Only process files that arrive after stream starts\\\n",
    "\\n   - Useful for: Initial data load or catching up\")\n",
    "\n",
    "\n",
    "print(\"\\n9️⃣ VALIDATION:\\\n",
    "\\n   .option('cloudFiles.validateOptions', 'true') ')\\\n",
    "\\n   - Validates all Auto Loader options at startup')\\\n",
    "\\n   - Catches configuration errors early\\\n",
    "\\n   - Recommended for production\")\n",
    "\n",
    "print(\"\\n🔟 PROVIDING SCHEMA:\\\n",
    "\\n   '.schema(multiplex_schema) \\\n",
    "\\n   - Provides explicit schema instead of full inference\\\n",
    "\\n   - Faster startup (no schema inference time)\\\n",
    "\\n   - More predictable behavior\\\n",
    "\\n   - Catches schema violations early\")\n",
    "\n",
    "print(\"\\n   💡 Schema vs Inference Trade-offs:\\\n",
    "\\n   With Explicit Schema:\\\n",
    "\\n   ✅ Faster startup\\\n",
    "\\n   ✅ Predictable structure \\\n",
    "\\n   ✅ Type safety\\\n",
    "\\n   ❌ Must maintain schema manually\")\n",
    "\n",
    "print(\"\\n   With Full Inference:\\\n",
    "\\n   ✅ Automatic discovery\\\n",
    "\\n   ✅ Handles unknown structures\\\n",
    "\\n   ❌ Slower first run\\\n",
    "\\n   ❌ Less predictable\")\n",
    "\n",
    "print(\"\\n1️⃣1️⃣ SOURCE PATH:\\\n",
    "\\n   ('.load(source_path)\\\n",
    "\\n   - WHERE: /Volumes/datastream_nexus/landing/raw_files/\\\n",
    "\\n   - WHAT: Directory to monitor for files\\\n",
    "\\n   - BEHAVIOR: Recursively monitors subdirectories\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "3"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 4510942662031554,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "RetailPlex_Source_Notebook",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
