{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "658185ef-ed24-4346-b435-a9f8a8bb6aab",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.window import Window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e83b007a-bbb4-4d85-955e-9b01edfca475",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Set Spark configs before any Spark actions\n",
    "spark.conf.set(\"spark.databricks.delta.optimizeWrite.enabled\", \"true\")\n",
    "spark.conf.set(\"spark.databricks.delta.autoCompact.enabled\", \"true\")\n",
    "spark.conf.set(\"spark.sql.shuffle.partitions\", \"200\")\n",
    "spark.conf.set(\"spark.sql.adaptive.enabled\", \"true\")\n",
    "spark.conf.set(\"spark.sql.adaptive.coalescePartitions.enabled\", \"true\")\n",
    "spark.conf.set(\"spark.sql.adaptive.skewJoin.enabled\", \"true\")\n",
    "spark.conf.set(\"spark.sql.streaming.schemaInference\", \"true\")\n",
    "spark.conf.set(\"spark.sql.broadcastTimeout\", \"1200\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "64398c67-c4b0-420b-b576-e8cd8317fad8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- Create schemas for medallion layers\n",
    "\n",
    "CREATE SCHEMA IF NOT EXISTS retailplex_platform.silver;\n",
    "\n",
    "-- Create volume for schema and checkpoints\n",
    "CREATE VOLUME IF NOT EXISTS retailplex_platform.silver.checkpoints\n",
    "COMMENT 'Volume for silver checkpoints';"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ae4ed67e-f779-4d75-a104-a9e002209a56",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Silver Schema - Customers Table\n",
    "customers_schema = \"\"\"\n",
    "customer_id STRING,\n",
    "first_name STRING,\n",
    "last_name STRING,\n",
    "email STRING,\n",
    "registration_date TIMESTAMP,\n",
    "customer_segment STRING,\n",
    "age INTEGER,\n",
    "gender STRING,\n",
    "city STRING,\n",
    "state STRING,\n",
    "country STRING,\n",
    "phone STRING\n",
    "\"\"\"\n",
    "\n",
    "products_schema = \"\"\"\n",
    "product_id STRING,\n",
    "product_name STRING,\n",
    "category STRING,\n",
    "subcategory STRING,\n",
    "brand STRING,\n",
    "price DECIMAL(10,2),\n",
    "cost DECIMAL(10,2),\n",
    "stock_quantity INTEGER,\n",
    "supplier_id STRING,\n",
    "launch_date TIMESTAMP,\n",
    "weight_kg DECIMAL(5,2)\n",
    "\"\"\"\n",
    "\n",
    "# Silver Schema - Orders Table\n",
    "orders_schema = \"\"\"\n",
    "order_id STRING,\n",
    "customer_id STRING,\n",
    "order_date TIMESTAMP,\n",
    "order_status STRING,\n",
    "total_amount DECIMAL(10,2),\n",
    "quantity INTEGER,\n",
    "discount_amount DECIMAL(10,2),\n",
    "tax_amount DECIMAL(10,2),\n",
    "shipping_cost DECIMAL(10,2),\n",
    "payment_method STRING,\n",
    "shipping_address STRING\n",
    "\"\"\"\n",
    "\n",
    "# Silver Schema - Events Table\n",
    "events_cdc_schema = \"\"\"\n",
    "event_id STRING,\n",
    "event_type STRING,\n",
    "customer_id STRING,\n",
    "product_id STRING,\n",
    "session_id STRING,\n",
    "page_url STRING,\n",
    "device_type STRING,\n",
    "browser STRING,\n",
    "ip_address STRING,\n",
    "referrer STRING,\n",
    "event_duration INTEGER\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6350a00d-3f5a-480d-ae63-8afa3387bd39",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "\n",
    "-- Drop table if EXISTS retailplex_platform.silver.product;\n",
    "CREATE TABLE IF NOT EXISTS retailplex_platform.silver.product (\n",
    "    product_id STRING,\n",
    "    product_name STRING,\n",
    "    category STRING,\n",
    "    subcategory STRING,\n",
    "    brand STRING,\n",
    "    price DECIMAL(10,2),\n",
    "    cost DECIMAL(10,2),\n",
    "    stock_quantity INT,\n",
    "    supplier_id STRING,\n",
    "    launch_date TIMESTAMP,\n",
    "    weight_kg DECIMAL(5,2),\n",
    "    processed_timestamp TIMESTAMP\n",
    "    )\n",
    "USING delta;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "43441613-1113-4488-9129-61273d8566b1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- Drop table if EXISTS retailplex_platform.silver.order;\n",
    "CREATE TABLE IF NOT EXISTS retailplex_platform.silver.order (\n",
    "    order_id STRING,\n",
    "    customer_id STRING,\n",
    "    order_date TIMESTAMP,\n",
    "    order_status STRING,\n",
    "    total_amount DECIMAL(10,2),\n",
    "    quantity INT,\n",
    "    discount_amount DECIMAL(10,2),\n",
    "    tax_amount DECIMAL(10,2),\n",
    "    shipping_cost DECIMAL(10,2),\n",
    "    payment_method STRING,\n",
    "    shipping_address STRING,\n",
    "    processed_timestamp TIMESTAMP\n",
    ")\n",
    "USING delta;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3395fe9f-73a7-4663-840d-180775008ee5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- Drop table if EXISTS retailplex_platform.silver.customer;\n",
    "CREATE TABLE IF NOT EXISTS retailplex_platform.silver.customer (\n",
    "  customer_id STRING,\n",
    "    first_name STRING,\n",
    "    last_name STRING,\n",
    "    email STRING,\n",
    "    registration_date TIMESTAMP,\n",
    "    customer_segment STRING,\n",
    "    age INT,\n",
    "    gender STRING,\n",
    "    city STRING,\n",
    "    state STRING,\n",
    "    country STRING,\n",
    "    phone STRING,\n",
    "    active_ind INTEGER,\n",
    "    valid_from TIMESTAMP,\n",
    "    valid_to TIMESTAMP\n",
    ")\n",
    "USING delta;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "988ab605-b7d3-4527-bdb1-b0fa35b6779c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "CREATE TABLE IF NOT EXISTS retailplex_platform.silver.event_cdc\n",
    "(\n",
    "  event_id STRING,\n",
    "  event_type STRING,\n",
    "  customer_id STRING,\n",
    "  product_id STRING,\n",
    "  session_id STRING,\n",
    "  page_url STRING,\n",
    "  device_type STRING,\n",
    "  browser STRING,\n",
    "  ip_address STRING,\n",
    "  referrer STRING,\n",
    "  event_duration INT\n",
    ")\n",
    "USING delta\n",
    "TBLPROPERTIES (delta.enableChangeDataFeed = true);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dda9dce9-a757-416d-ab55-e1a72e151850",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "CREATE TABLE IF NOT EXISTS retailplex_platform.silver.event\n",
    "(\n",
    "  event_id STRING,\n",
    "  event_type STRING,\n",
    "  customer_id STRING,\n",
    "  product_id STRING,\n",
    "  session_id STRING,\n",
    "  page_url STRING,\n",
    "  device_type STRING,\n",
    "  browser STRING,\n",
    "  ip_address STRING,\n",
    "  referrer STRING,\n",
    "  event_duration INT,\n",
    "  processed_timestamp TIMESTAMP\n",
    ")\n",
    "USING delta;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5976004c-ad32-4f90-a3ff-63988a48b2aa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Step 1: Read from Bronze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8eb1cf5f-ef66-44ce-b81c-ad2c0a75a81e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "bronzeDF = (spark.readStream\n",
    "    .table(\"retailplex_platform.bronze.multiplex_stream\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "58bafa4f-cfd9-4e8e-86df-820c1b07ac1d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Step 2: Order data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9e5296c4-2988-4fcd-9e03-823aa1b27488",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Define the merge function for foreachBatch\n",
    "def merge_batch_order(batchDF, batchId):\n",
    "    # Register the deduplicated batch as a temp view for SQL\n",
    "    batchDF.createOrReplaceTempView(\"batch_orders\")\n",
    "    \n",
    "    # Perform MERGE INTO using SQL\n",
    "    spark_sql = (\"\"\"\n",
    "        MERGE INTO retailplex_platform.silver.order AS target\n",
    "        USING batch_orders AS source\n",
    "        ON target.order_id = source.order_id AND target.order_date = source.order_date\n",
    "        WHEN MATCHED THEN\n",
    "            UPDATE SET\n",
    "                target.customer_id = source.customer_id,\n",
    "                target.order_status = source.order_status,\n",
    "                target.total_amount = source.total_amount,\n",
    "                target.quantity = source.quantity,\n",
    "                target.discount_amount  = source.discount_amount ,\n",
    "                target.tax_amount  = source.tax_amount ,\n",
    "                target.shipping_cost  = source.shipping_cost ,\n",
    "                target.payment_method  = source.payment_method ,\n",
    "                target.shipping_address  = source.shipping_address ,\n",
    "                target.processed_timestamp = source.processed_timestamp\n",
    "        WHEN NOT MATCHED THEN\n",
    "            INSERT (\n",
    "                order_id,\n",
    "                customer_id,\n",
    "                order_date,\n",
    "                order_status,\n",
    "                total_amount,\n",
    "                quantity,\n",
    "                discount_amount,\n",
    "                tax_amount,\n",
    "                shipping_cost,\n",
    "                payment_method,\n",
    "                shipping_address,\n",
    "                processed_timestamp\n",
    "            )\n",
    "            VALUES (\n",
    "                source.order_id,\n",
    "                source.customer_id,\n",
    "                source.order_date,\n",
    "                source.order_status,\n",
    "                source.total_amount,\n",
    "                source.quantity,\n",
    "                source.discount_amount,\n",
    "                source.tax_amount,\n",
    "                source.shipping_cost,\n",
    "                source.payment_method,\n",
    "                source.shipping_address,\n",
    "                source.processed_timestamp\n",
    "            );\n",
    "    \"\"\")\n",
    "    batchDF.sparkSession.sql(spark_sql)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "39c059c4-bf62-4638-8d39-991bc883abbf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def process_order_data():\n",
    "    bronzeDF.createOrReplaceTempView(\"bronze_product_stream\")\n",
    "\n",
    "    # Parse the JSON data and filter for orders topic using SQL\n",
    "    ordersDF = bronzeDF.filter(\"topic = 'orders'\")\\\n",
    "        .select(F.from_json(F.col(\"data\").cast(\"string\"), orders_schema).alias(\"order\")) \\\n",
    "        .select(\"order.*\", F.current_timestamp().alias(\"processed_timestamp\") )\\\n",
    "        .withWatermark(\"order_date\", \"10 seconds\")\\\n",
    "        .dropDuplicates([\"order_id\", \"order_date\"])\n",
    "\n",
    "    ordersQuery = (ordersDF.writeStream\n",
    "    .format(\"delta\")\n",
    "    .option(\"checkpointLocation\", \"/Volumes/retailplex_platform/silver/checkpoints/order\")\n",
    "    .trigger(availableNow=True)\n",
    "    .foreachBatch(merge_batch_order)\n",
    "    .start())\n",
    "\n",
    "    ordersQuery.awaitTermination()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0bacb944-8158-4f2f-951b-a2788925dc3c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "process_order_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "db9c257d-d3ce-4480-a2a7-aa233ff3ec86",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Step 2: Product data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7f75cc33-3a06-4687-8328-b1f51dbe85e6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def merge_batch_product(batchDF, batchId):\n",
    "    # Register batch dataframe as a temp view for SQL\n",
    "    batchDF.createOrReplaceTempView(\"raw_batch_products\")\n",
    "    \n",
    "    spark = batchDF.sparkSession\n",
    "    \n",
    "    # Deduplicate source by product_id keeping latest record if duplicates exist\n",
    "    spark.sql(\"\"\"\n",
    "    CREATE OR REPLACE TEMP VIEW batch_products AS\n",
    "    SELECT product_id,\n",
    "           product_name,\n",
    "           category,\n",
    "           brand,\n",
    "           price,\n",
    "           cost,\n",
    "           stock_quantity,\n",
    "           supplier_id,\n",
    "           launch_date,\n",
    "           weight_kg,\n",
    "           processed_timestamp\n",
    "    FROM (\n",
    "      SELECT *,\n",
    "        ROW_NUMBER() OVER (PARTITION BY product_id ORDER BY processed_timestamp DESC) AS rn\n",
    "      FROM raw_batch_products\n",
    "    ) dedup\n",
    "    WHERE rn = 1\n",
    "    \"\"\")\n",
    "\n",
    "    # Now perform the merge using the deduplicated view\n",
    "    spark.sql(\"\"\"\n",
    "        MERGE INTO retailplex_platform.silver.product AS target\n",
    "        USING batch_products AS source\n",
    "        ON target.product_id = source.product_id\n",
    "        WHEN MATCHED THEN\n",
    "            UPDATE SET\n",
    "                target.product_name = source.product_name,\n",
    "                target.category = source.category,\n",
    "                target.brand = source.brand,\n",
    "                target.price = source.price,\n",
    "                target.cost = source.cost,\n",
    "                target.stock_quantity = source.stock_quantity,\n",
    "                target.supplier_id = source.supplier_id,\n",
    "                target.launch_date = source.launch_date,\n",
    "                target.weight_kg = source.weight_kg,\n",
    "                target.processed_timestamp = source.processed_timestamp\n",
    "        WHEN NOT MATCHED THEN\n",
    "            INSERT (\n",
    "                product_id,\n",
    "                product_name,\n",
    "                category,\n",
    "                brand,\n",
    "                price,\n",
    "                cost,\n",
    "                stock_quantity,\n",
    "                supplier_id,\n",
    "                launch_date,\n",
    "                weight_kg,\n",
    "                processed_timestamp\n",
    "            )\n",
    "            VALUES (\n",
    "                source.product_id,\n",
    "                source.product_name,\n",
    "                source.category,\n",
    "                source.brand,\n",
    "                source.price,\n",
    "                source.cost,\n",
    "                source.stock_quantity,\n",
    "                source.supplier_id,\n",
    "                source.launch_date,\n",
    "                source.weight_kg,\n",
    "                source.processed_timestamp\n",
    "            )\n",
    "    \"\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "430bc6b9-583f-4dab-a16d-167840613f3a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def process_product_data():\n",
    "    bronzeDF.createOrReplaceTempView(\"bronze_product_stream\")\n",
    "\n",
    "    # Parse the JSON data and filter for orders topic using SQL\n",
    "    productsDF = spark.sql(f\"\"\"\n",
    "        SELECT products.*, current_timestamp() AS processed_timestamp\n",
    "        FROM (\n",
    "            SELECT from_json(cast(data AS string), '{products_schema}') AS products\n",
    "            FROM bronze_product_stream\n",
    "            WHERE topic = 'products'\n",
    "        )\n",
    "    \"\"\")\n",
    "\n",
    "    productsQuery = (productsDF.writeStream\n",
    "    .format(\"delta\")\n",
    "    .option(\"checkpointLocation\", \"/Volumes/retailplex_platform/silver/checkpoints/product\")\n",
    "    .trigger(availableNow=True)\n",
    "    .foreachBatch(merge_batch_product)\n",
    "    .start())\n",
    "\n",
    "    productsQuery.awaitTermination()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5b2efaeb-a766-422b-b0d2-978ba2a39efc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "process_product_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c6bb0b84-c3e5-433a-beff-8dcdd3835957",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Step 2: Customer data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "012a9e68-ece3-4e7c-a28c-2b2c513d172e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Define the merge function for foreachBatch\n",
    "def merge_batch_customer(batchDF, batchId):\n",
    "    # Register batch dataframe as a temp view for SQL usage\n",
    "    batchDF.createOrReplaceTempView(\"batch_customers\")\n",
    "    \n",
    "    # Use spark session from batchDF\n",
    "    spark = batchDF.sparkSession\n",
    "    \n",
    "    # Create staged_updates view (one time)\n",
    "    spark.sql(\"\"\"\n",
    "    CREATE OR REPLACE TEMP VIEW staged_updates AS\n",
    "    SELECT customer_id,\n",
    "           first_name,\n",
    "           last_name,\n",
    "           email,\n",
    "           registration_date,\n",
    "           customer_segment,\n",
    "           age,\n",
    "           gender,\n",
    "           city,\n",
    "           state,\n",
    "           country,\n",
    "           phone,\n",
    "           current_timestamp() AS valid_from\n",
    "    FROM (\n",
    "      SELECT *,\n",
    "        ROW_NUMBER() OVER (PARTITION BY customer_id ORDER BY current_timestamp() DESC) AS rn\n",
    "      FROM batch_customers\n",
    "    ) dedup\n",
    "    WHERE rn = 1\n",
    "    \"\"\")\n",
    "    \n",
    "     # Step 1: Expire old active records that have changed\n",
    "    spark.sql(\"\"\"\n",
    "    MERGE INTO retailplex_platform.silver.customer AS target\n",
    "    USING staged_updates AS source\n",
    "    ON target.customer_id = source.customer_id\n",
    "       AND target.active_ind = 1\n",
    "        AND (\n",
    "         target.first_name <> source.first_name OR\n",
    "         target.last_name <> source.last_name OR\n",
    "         target.email <> source.email OR\n",
    "         target.registration_date <> source.registration_date OR\n",
    "         target.customer_segment <> source.customer_segment OR\n",
    "         target.age <> source.age OR\n",
    "         target.gender <> source.gender OR\n",
    "         target.city <> source.city OR\n",
    "         target.state <> source.state OR\n",
    "         target.country <> source.country OR\n",
    "         target.phone <> source.phone\n",
    "       )\n",
    "    WHEN MATCHED THEN\n",
    "      UPDATE SET active_ind = 0,\n",
    "                 valid_to = current_timestamp()\n",
    "    \"\"\")\n",
    "\n",
    "        # Step 2: Insert new changed and new rows (active records)\n",
    "    # Insert new changed and new records which are not present as active rows anymore\n",
    "    spark.sql(\"\"\"\n",
    "    INSERT INTO retailplex_platform.silver.customer\n",
    "    SELECT\n",
    "      su.customer_id,\n",
    "      su.first_name,\n",
    "      su.last_name,\n",
    "      su.email,\n",
    "      su.registration_date,\n",
    "      su.customer_segment,\n",
    "      su.age,\n",
    "      su.gender,\n",
    "      su.city,\n",
    "      su.state,\n",
    "      su.country,\n",
    "      su.phone,\n",
    "      1 AS active_ind,\n",
    "      current_timestamp() AS valid_from,\n",
    "      NULL AS valid_to\n",
    "    FROM staged_updates su\n",
    "    LEFT JOIN retailplex_platform.silver.customer t\n",
    "      ON su.customer_id = t.customer_id\n",
    "      AND t.active_ind = 1\n",
    "    WHERE t.customer_id IS NULL\n",
    "       OR (\n",
    "         t.first_name <> su.first_name OR\n",
    "         t.last_name <> su.last_name OR\n",
    "         t.email <> su.email OR\n",
    "         t.registration_date <> su.registration_date OR\n",
    "         t.customer_segment <> su.customer_segment OR\n",
    "         t.age <> su.age OR\n",
    "         t.gender <> su.gender OR\n",
    "         t.city <> su.city OR\n",
    "         t.state <> su.state OR\n",
    "         t.country <> su.country OR \n",
    "         t.phone <> su.phone\n",
    "       )\n",
    "    \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0ef0afef-91ad-4ba5-b739-92119e506dd6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def process_customer_data():\n",
    "    bronzeDF.createOrReplaceTempView(\"bronze_customer_stream\")\n",
    "    customersDF = spark.sql(f\"\"\"\n",
    "    SELECT customers.*, \n",
    "            1 as active_ind,\n",
    "           current_timestamp() AS valid_from, \n",
    "           NULL AS valid_to\n",
    "    FROM (\n",
    "        SELECT from_json(cast(data AS string), '{customers_schema}') AS customers\n",
    "        FROM bronze_customer_stream\n",
    "        WHERE topic = 'customers'\n",
    "    )\n",
    "        \"\"\")\n",
    "\n",
    "    customersQuery = (customersDF.writeStream\n",
    "        .format(\"delta\")\n",
    "        .option(\"checkpointLocation\", \"/Volumes/retailplex_platform/silver/checkpoints/customer\")\n",
    "        .trigger(availableNow=True)\n",
    "        .foreachBatch(merge_batch_customer)\n",
    "        .start())\n",
    "\n",
    "    customersQuery.awaitTermination()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "71379d37-90dd-4cc9-b467-436b8dee947c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "process_customer_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b01220d7-2a70-4871-9aa5-ada4dc774a8c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Step 2: Event_CDC data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c4d54d82-e7af-400e-b8f6-080a868f809d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def merge_events_cdc(batch_df, batch_id):\n",
    "    if batch_df.count() > 0:\n",
    "        # Create a temporary view from the batch DataFrame\n",
    "        batch_df.createOrReplaceTempView(\"batch_events\")\n",
    "        \n",
    "        # Execute SQL MERGE INTO statement\n",
    "        spark_sql = (\"\"\"\n",
    "                MERGE INTO retailplex_platform.silver.event_cdc AS target\n",
    "                USING batch_events AS source\n",
    "                ON target.event_id = source.event_id\n",
    "                WHEN MATCHED THEN\n",
    "                    UPDATE SET\n",
    "                        event_type = source.event_type,\n",
    "                        customer_id = source.customer_id,\n",
    "                        product_id = source.product_id,\n",
    "                        session_id = source.session_id,\n",
    "                        page_url = source.page_url,\n",
    "                        device_type = source.device_type,\n",
    "                        browser = source.browser,\n",
    "                        ip_address = source.ip_address,\n",
    "                        referrer = source.referrer,\n",
    "                        event_duration = source.event_duration\n",
    "                WHEN NOT MATCHED THEN\n",
    "                    INSERT (event_id, event_type, customer_id, product_id, session_id, \n",
    "                        page_url, device_type, browser, ip_address, referrer, event_duration)\n",
    "                    VALUES (source.event_id, source.event_type, source.customer_id, \n",
    "                        source.product_id, source.session_id, source.page_url,\n",
    "                        source.device_type, source.browser, source.ip_address, \n",
    "                        source.referrer, source.event_duration)\n",
    "            \"\"\")\n",
    "\n",
    "    batch_df.sparkSession.sql(spark_sql)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e5d2ca19-8f8f-4d7d-b1ed-5ec6744d689f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def process_event_cdc_data():\n",
    "    eventsDF = bronzeDF.filter(\"topic = 'events'\")\\\n",
    "    .select(F.from_json(F.col(\"data\").cast(\"string\"), events_cdc_schema)\n",
    "            .alias(\"events\"))\\\n",
    "    .select(\"events.*\")\n",
    "\n",
    "    eventsQuery = (eventsDF.writeStream\n",
    "                .format(\"delta\")\n",
    "                .foreachBatch(merge_events_cdc)\n",
    "                .option(\"checkpointLocation\", \"/Volumes/retailplex_platform/silver/checkpoints/event_cdc\")\n",
    "                .trigger(availableNow=True)\n",
    "                .start())\n",
    "\n",
    "    eventsQuery.awaitTermination()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "900cac4e-e2ad-4ad8-8d39-1636e8e97122",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "process_event_cdc_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "750febe4-8013-47e3-a076-ee4bad8e0d96",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Step 2: Event data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9eec2c6c-8816-4029-ba17-cbd88d48c828",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Define the merge function for foreachBatch\n",
    "def merge_latest_events_batch(batchDF, batchId):\n",
    "    # Register the deduplicated batch as a temp view for SQL\n",
    "\n",
    "\n",
    "    window = Window.partitionBy(\"event_id\").orderBy(F.col(\"_commit_version\").desc())\n",
    "\n",
    "    (batchDF.filter(F.col(\"_change_type\").isin([\"insert\", \"update_postimage\",\"delete\"])) \n",
    "        .withColumn(\"rank\", F.rank().over(window))\n",
    "        .filter(\"rank = 1\") \n",
    "        .drop(\"rank\")\n",
    "        .withColumnRenamed(\"_commit_timestamp\", \"processed_timestamp\")\n",
    "        .createOrReplaceTempView(\"batch_events\"))\n",
    "    \n",
    "    # Perform MERGE INTO using SQL\n",
    "    spark_sql = (\"\"\"\n",
    "        MERGE INTO retailplex_platform.silver.event AS target\n",
    "        USING batch_events AS source\n",
    "        ON target.event_id = source.event_id \n",
    "        WHEN MATCHED AND source._change_type != 'delete'\n",
    "        AND target.processed_timestamp < source.processed_timestamp\n",
    "         THEN\n",
    "            UPDATE SET\n",
    "                target.event_type = source.event_type,\n",
    "                target.customer_id = source.customer_id,\n",
    "                target.product_id = source.product_id,\n",
    "                target.session_id = source.session_id,\n",
    "                target.page_url  = source.page_url ,\n",
    "                target.device_type  = source.device_type ,\n",
    "                target.browser  = source.browser ,\n",
    "                target.ip_address  = source.ip_address ,\n",
    "                target.referrer  = source.referrer ,\n",
    "                target.event_duration  = source.event_duration ,\n",
    "                target.processed_timestamp = current_timestamp()\n",
    "        WHEN MATCHED AND source._change_type = 'delete' THEN\n",
    "            DELETE\n",
    "        WHEN NOT MATCHED AND source._change_type != 'delete' THEN\n",
    "            INSERT (\n",
    "                event_id,\n",
    "                event_type,\n",
    "                customer_id,\n",
    "                product_id,\n",
    "                session_id,\n",
    "                page_url,\n",
    "                device_type,\n",
    "                browser,\n",
    "                ip_address,\n",
    "                referrer,\n",
    "                event_duration,\n",
    "                processed_timestamp\n",
    "            )\n",
    "            VALUES (\n",
    "                source.event_id,\n",
    "                source.event_type,\n",
    "                source.customer_id,\n",
    "                source.product_id,\n",
    "                source.session_id,\n",
    "                source.page_url,\n",
    "                source.device_type,\n",
    "                source.browser,\n",
    "                source.ip_address,\n",
    "                source.referrer,\n",
    "                source.event_duration,\n",
    "                current_timestamp()\n",
    "            );\n",
    "    \"\"\")\n",
    "    batchDF.sparkSession.sql(spark_sql)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1365bcdd-0787-41ee-914d-03e361bfb628",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def process_event_data():\n",
    "    cdc_stream = (spark.readStream\n",
    "    .option(\"readChangeFeed\", \"true\")  # Stream changes via CDC\n",
    "    .option(\"startingVersion\", 0)  # Start from version 0; \n",
    "    .table(\"retailplex_platform.silver.event_cdc\"))\n",
    "\n",
    "    query = (cdc_stream.writeStream\n",
    "    .format(\"delta\")\n",
    "    .outputMode(\"update\")  # Append changes to batch\n",
    "    .option(\"checkpointLocation\", \"/Volumes/retailplex_platform/silver/checkpoints/event\")\n",
    "    .trigger(availableNow=True)\n",
    "    .foreachBatch(merge_latest_events_batch)\n",
    "    .start())\n",
    "\n",
    "    query.awaitTermination()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5b19c205-9bad-4cd5-b6bd-65160eaa8198",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "process_event_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "925f3d03-7af8-4819-90fc-c84deb16a5d2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "USE CATALOG retailplex_platform;\n",
    "\n",
    "\n",
    "CREATE OR REPLACE VIEW silver.broadcast_customer_segments AS\n",
    "SELECT\n",
    "  CAST(segment_id AS INT)                               AS segment_id,\n",
    "  TRIM(segment_name)                                    AS segment_name,\n",
    "  TRIM(segment_description)                             AS segment_description,\n",
    "  CAST(min_spend_threshold AS DECIMAL(18,2))            AS min_spend_threshold,\n",
    "  CAST(discount_percentage AS DECIMAL(5,2))             AS discount_percentage,\n",
    "  CAST(priority_support AS BOOLEAN)                     AS priority_support,\n",
    "  CAST(free_shipping_threshold AS DECIMAL(18,2))        AS free_shipping_threshold,\n",
    "  CURRENT_TIMESTAMP()                                   AS _ingested_at\n",
    "FROM bronze.customer_segments\n",
    "QUALIFY ROW_NUMBER() OVER (PARTITION BY segment_id ORDER BY segment_id) = 1;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cfb0614b-6651-46c2-85df-2c0739f77f60",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "CREATE OR REPLACE VIEW silver.broadcast_product_categories AS\n",
    "SELECT\n",
    "  CAST(category_id AS INT)                   AS category_id,\n",
    "  TRIM(category_name)                        AS category_name,\n",
    "  TRIM(category_description)                 AS category_description,\n",
    "  CAST(commission_rate AS DECIMAL(5,4))      AS commission_rate,\n",
    "  CAST(tax_rate AS DECIMAL(5,4))             AS tax_rate,\n",
    "  CAST(return_policy_days AS INT)            AS return_policy_days,\n",
    "  CURRENT_TIMESTAMP()                        AS _ingested_at\n",
    "FROM bronze.product_categories\n",
    "QUALIFY ROW_NUMBER() OVER (PARTITION BY category_id ORDER BY category_id) = 1;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9f6df5c1-a581-47c4-adfe-4e0e7aec75df",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "CREATE OR REPLACE VIEW silver.broadcast_product_subcategories AS\n",
    "SELECT\n",
    "  CAST(subcategory_id AS INT)            AS subcategory_id,\n",
    "  TRIM(subcategory_name)                 AS subcategory_name,\n",
    "  CAST(category_id AS INT)               AS category_id,\n",
    "  TRIM(subcategory_description)          AS subcategory_description,\n",
    "  TRIM(storage_requirements)             AS storage_requirements,\n",
    "  CAST(fragile_flag AS BOOLEAN)          AS fragile_flag,\n",
    "  CURRENT_TIMESTAMP()                    AS _ingested_at\n",
    "FROM bronze.product_subcategories\n",
    "QUALIFY ROW_NUMBER() OVER (PARTITION BY subcategory_id ORDER BY subcategory_id) = 1;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dea607d0-b8bb-4455-9415-fb10304f033e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "CREATE OR REPLACE VIEW silver.broadcast_suppliers AS\n",
    "SELECT\n",
    "  TRIM(supplier_id)                        AS supplier_id,\n",
    "  TRIM(supplier_name)                      AS supplier_name,\n",
    "  TRIM(supplier_country)                   AS supplier_country,\n",
    "  TRIM(supplier_region)                    AS supplier_region,\n",
    "  TRIM(contact_email)                      AS contact_email,\n",
    "  CAST(payment_terms_days AS INT)          AS payment_terms_days,\n",
    "  CAST(quality_rating AS DECIMAL(3,1))     AS quality_rating,\n",
    "  CAST(delivery_time_days AS INT)          AS delivery_time_days,\n",
    "  CAST(minimum_order_value AS DECIMAL(18,2)) AS minimum_order_value,\n",
    "  CURRENT_TIMESTAMP()                      AS _ingested_at\n",
    "FROM bronze.suppliers\n",
    "QUALIFY ROW_NUMBER() OVER (PARTITION BY supplier_id ORDER BY supplier_id) = 1;\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3aca592b-5063-4c08-b0e5-0834d1318315",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "CREATE OR REPLACE VIEW silver.geography AS\n",
    "SELECT\n",
    "  UPPER(TRIM(state_code))        AS state_code,\n",
    "  TRIM(state_name)               AS state_name,\n",
    "  TRIM(region)                   AS region,\n",
    "  UPPER(TRIM(country_code))      AS country_code,\n",
    "  TRIM(country_name)             AS country_name,\n",
    "  TRIM(timezone)                 AS timezone,\n",
    "  CAST(sales_tax_rate AS DECIMAL(5,4)) AS sales_tax_rate,\n",
    "  CAST(shipping_zone AS INT)     AS shipping_zone,\n",
    "  CAST(population_millions AS DECIMAL(6,1)) AS population_millions,\n",
    "  CURRENT_TIMESTAMP()            AS _ingested_at\n",
    "FROM bronze.geography\n",
    "QUALIFY ROW_NUMBER() OVER (PARTITION BY state_code ORDER BY state_code) = 1;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "66a22208-5ae0-4304-b650-52c24e613902",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1756399260426}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "select * from retailplex_platform.silver.customer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c0440e20-5d23-4d0b-9a97-48d7bf46c6e7",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{\"_commit_version\":163},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1756560654970}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "select * FROM table_changes('retailplex_platform.silver.event_cdc', 0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c4cb5173-c1e9-4555-8788-4b96eacca41a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    " drop table if exists retailplex_platform.silver.event_cdc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "45d723a2-b4b8-4b28-9e12-366e251014fa",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1756460744377}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "select * from retailplex_platform.bronze.multiplex_stream "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "073fef68-5ba7-4b53-bfbc-b3e557902a54",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "latest_version = spark.sql(f\"SELECT max(version) FROM (DESCRIBE HISTORY delta.`{first_silver_path}`)\").collect()[0][0]"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "3"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 8793260270361613,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "bronze_to_silver_streaming",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
