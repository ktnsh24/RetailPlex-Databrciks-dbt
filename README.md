# RetailPlex-Databrciks-dbt

This project simulates a **retail analytics platform** where data is ingested from multiple sources (streaming, batch, and reference data) and staged in **Databricks Unity Catalog** under the catalog `retailplex_platform` and schema `landing`.  

---

## ðŸ“Œ Data Sources

### 1. Streaming Multiplex Data
- Arrives as **JSONL** files under:  

/Volumes/retailplex_platform/landing/raw_files/incoming_multiplex_data/retailplex_multiplex_stream_<timestamp>.jsonl

- Each file contains multiple **topics**:
- `customers` â†’ customer profile data  
- `products` â†’ product master data  
- `orders` â†’ customer orders  
- `events` â†’ customer browsing and interaction events  

---

### 2. Reference Data (Broadcast Tables)
- Small, relatively static lookup/enrichment datasets in **CSV format**.  
- Stored under:  

/Volumes/retailplex_platform/landing/raw_files/refdata/

- Includes:
- `Customer_segments.csv`
- `Geography.csv`
- `Product_categories.csv`
- `Product_subcategories.csv`
- `Suppliers.csv`

---

### 3. Batch Data (Singleplex Full Load)
- Large transactional dataset refreshed **daily as a full load**.  
- Stored under:  

/Volumes/retailplex_platform/landing/raw_files/batch_data/batch_order_items.csv

- Loaded as managed table: landing.batch_order_items.
- Provides **order lineâ€“level transactions** linking customers, orders, products, and events.  

---
So far, the landing schema acts as the raw ingestion zone:

- Streaming JSON lines (multiplex)
- Referential CSVs (refdata)
- Bulk batch CSV (batch_order_items)

These datasets are the foundation for downstream bronze â†’ silver â†’ gold processing in the medallion architecture.

## ðŸ“‚ Data Flow till Landing Schema


```plaintext
retailplex_platform/
â”œâ”€â”€ landing/
â”‚   â”œâ”€â”€ raw_files/
â”‚   â”‚   â”œâ”€â”€ incoming_multiplex_data/
â”‚   â”‚   â”‚   â””â”€â”€ retailplex_multiplex_stream_<timestamp>.jsonl   # Streaming data
â”‚   â”‚   â”œâ”€â”€ refdata/                                            # Reference / broadcast data
â”‚   â”‚   â”‚   â”œâ”€â”€ Customer_segments.csv
â”‚   â”‚   â”‚   â”œâ”€â”€ Geography.csv
â”‚   â”‚   â”‚   â”œâ”€â”€ Product_categories.csv
â”‚   â”‚   â”‚   â”œâ”€â”€ Product_subcategories.csv
â”‚   â”‚   â”‚   â””â”€â”€ Suppliers.csv
â”‚   â”‚   â””â”€â”€ batch_data/
â”‚   â”‚       â””â”€â”€ batch_order_items.csv                           # Daily full load
â”‚   â””â”€â”€ tables/
â”‚       â””â”€â”€ batch_order_items                                   # Landing managed table
```
---

## âœ… Key Understanding
- **Multiplex streaming JSONL** â†’ multiple topics intermixed, need to be split into separate raw Bronze tables.  
- **Broadcast/reference CSVs** â†’ lookup dimensions for enrichment and joins.  
- **Batch CSV (order items)** â†’ fact-level transactional data, refreshed daily.  
- Together, these represent the **raw landing zone** of the Medallion Architecture before transformation into Bronze â†’ Silver â†’ Gold layers.  

## ðŸ¥‰ Bronze Layer (Medallion Architecture)

In the **Bronze schema**, data from the Landing layer is ingested into structured tables while still retaining raw characteristics.  
The key principle here is to **preserve the original data** but add minimal metadata such as ingestion timestamps.

### ðŸ”¹ Data Ingestion Strategy

- **Streaming Data (Multiplex Stream)**  
  - Data from `retailplex_multiplex_stream_<timestamp>.jsonl` is ingested into the Bronze table `multiplex_stream`.  
  - Implemented using **Spark Structured Streaming** (`spark.readStream` + `spark.writeStream`).  
  - A column `_ingestion_timestamp` is added (`current_timestamp()` at ingestion).  
  - Streaming checkpoints are managed in `/Volumes/checkpoints/retailplex_stream`.

- **Batch Data (DBT âš¡)**  
  - `batch_order_items.csv` (large full load dataset) is processed via **dbt**.  
  - Materialized into the Bronze schema as `bronze_batch_order_items`.

- **Broadcast Reference Tables (CSV-based)**  
  - `Customer_segments`, `Product_categories`, `Product_subcategories`, and `Suppliers` are ingested via **COPY INTO** commands in Databricks.  
  - These are relatively small lookup/reference datasets.

- **Broadcast_Geography Data (DBT Seed âš¡)**  
  - The `broadcast_geography.csv` dataset is managed as a **dbt seed**.  
  - Materialized directly in the Bronze schema.

---

### ðŸ”¹ Bronze Layer Flow

```plaintext
retailplex_platform/
â”œâ”€â”€ landing/
â”‚   â””â”€â”€ raw_files/
â”‚       â”œâ”€â”€ incoming_multiplex_data/ (multiplex JSONL stream)
â”‚       â”œâ”€â”€ batch_data/ (batch_order_items.csv)
â”‚       â””â”€â”€ refdata/ (reference CSVs)
â”‚
â”œâ”€â”€ bronze/
â”‚   â”œâ”€â”€ multiplex_stream                  (streamed via Spark Structured Streaming + ingestion_timestamp)
â”‚   â”œâ”€â”€ bronze_batch_order_items âš¡       (materialized via dbt model from batch_order_items.csv)
â”‚   â”œâ”€â”€ customer_segments                 (COPY INTO from refdata)
â”‚   â”œâ”€â”€ product_categories                (COPY INTO from refdata)
â”‚   â”œâ”€â”€ product_subcategories             (COPY INTO from refdata)
â”‚   â”œâ”€â”€ suppliers                         (COPY INTO from refdata)
â”‚   â”œâ”€â”€ geography âš¡                       (dbt seed â†’ materialized in Bronze)
â”‚   â””â”€â”€ (checkpoints for streaming maintained under Volumes/checkpoints/retailplex_stream)
âš¡ = Indicates dbt involvement
```
