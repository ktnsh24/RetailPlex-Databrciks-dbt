# RetailPlex-Databrciks-dbt

This project simulates a **retail analytics platform** where data is ingested from multiple sources (streaming, batch, and reference data) and staged in **Databricks Unity Catalog** under the catalog `retailplex_platform` and schema `landing`.  

---

## ğŸ“Œ Data Sources

### 1. Streaming Multiplex Data
- Arrives as **JSONL** files under:  

/Volumes/retailplex_platform/landing/raw_files/incoming_multiplex_data/retailplex_multiplex_stream_<timestamp>.jsonl

- Each file contains multiple **topics**:
- `customers` â†’ customer profile data  
- `products` â†’ product master data  
- `orders` â†’ customer orders  
- `events` â†’ customer browsing and interaction events  

---

### 2. Reference Data (Broadcast Tables)
- Small, relatively static lookup/enrichment datasets in **CSV format**.  
- Stored under:  

/Volumes/retailplex_platform/landing/raw_files/refdata/

- Includes:
- `Customer_segments.csv`
- `Geography.csv`
- `Product_categories.csv`
- `Product_subcategories.csv`
- `Suppliers.csv`

---

### 3. Batch Data (Singleplex Full Load)
- Large transactional dataset refreshed **daily as a full load**.  
- Stored under:  

/Volumes/retailplex_platform/landing/raw_files/batch_data/batch_order_items.csv

- Loaded as managed table: landing.batch_order_items.
- Provides **order lineâ€“level transactions** linking customers, orders, products, and events.  

---
So far, the landing schema acts as the raw ingestion zone:

- Streaming JSON lines (multiplex)
- Referential CSVs (refdata)
- Bulk batch CSV (batch_order_items)

These datasets are the foundation for downstream bronze â†’ silver â†’ gold processing in the medallion architecture.

## ğŸ“‚ Data Flow till Landing Schema


```plaintext
retailplex_platform/
â”œâ”€â”€ landing/
â”‚   â”œâ”€â”€ raw_files/
â”‚   â”‚   â”œâ”€â”€ incoming_multiplex_data/
â”‚   â”‚   â”‚   â””â”€â”€ retailplex_multiplex_stream_<timestamp>.jsonl   # Streaming data
â”‚   â”‚   â”œâ”€â”€ refdata/                                            # Reference / broadcast data
â”‚   â”‚   â”‚   â”œâ”€â”€ Customer_segments.csv
â”‚   â”‚   â”‚   â”œâ”€â”€ Geography.csv
â”‚   â”‚   â”‚   â”œâ”€â”€ Product_categories.csv
â”‚   â”‚   â”‚   â”œâ”€â”€ Product_subcategories.csv
â”‚   â”‚   â”‚   â””â”€â”€ Suppliers.csv
â”‚   â”‚   â””â”€â”€ batch_data/
â”‚   â”‚       â””â”€â”€ batch_order_items.csv                           # Daily full load
â”‚   â””â”€â”€ tables/
â”‚       â””â”€â”€ batch_order_items                                   # Landing managed table
```
---

## âœ… Key Understanding
- **Multiplex streaming JSONL** â†’ multiple topics intermixed, need to be split into separate raw Bronze tables.  
- **Broadcast/reference CSVs** â†’ lookup dimensions for enrichment and joins.  
- **Batch CSV (order items)** â†’ fact-level transactional data, refreshed daily.  
- Together, these represent the **raw landing zone** of the Medallion Architecture before transformation into Bronze â†’ Silver â†’ Gold layers.  

## ğŸ¥‰ Bronze Layer (Medallion Architecture)

In the **Bronze schema**, data from the Landing layer is ingested into structured tables while still retaining raw characteristics.  
The key principle here is to **preserve the original data** but add minimal metadata such as ingestion timestamps.

### ğŸ”¹ Data Ingestion Strategy

- **Streaming Data (Multiplex Stream)**  
  - Data from `retailplex_multiplex_stream_<timestamp>.jsonl` is ingested into the Bronze table `multiplex_stream`.  
  - Implemented using **Spark Structured Streaming** (`spark.readStream` + `spark.writeStream`).  
  - A column `_ingestion_timestamp` is added (`current_timestamp()` at ingestion).  
  - Streaming checkpoints are managed in `/Volumes/checkpoints/retailplex_stream`.

- **Batch Data (DBT âš¡)**  
  - `batch_order_items.csv` (large full load dataset) is processed via **dbt**.  
  - Materialized into the Bronze schema as `bronze_batch_order_items`.

- **Broadcast Reference Tables (CSV-based)**  
  - `Customer_segments`, `Product_categories`, `Product_subcategories`, and `Suppliers` are ingested via **COPY INTO** commands in Databricks.  
  - These are relatively small lookup/reference datasets.

- **Broadcast_Geography Data (DBT Seed âš¡)**  
  - The `broadcast_geography.csv` dataset is managed as a **dbt seed**.  
  - Materialized directly in the Bronze schema.

---

### ğŸ”¹ Bronze Layer Flow

```plaintext
retailplex_platform/
â”œâ”€â”€ landing/
â”‚   â””â”€â”€ raw_files/
â”‚       â”œâ”€â”€ incoming_multiplex_data/ (multiplex JSONL stream)
â”‚       â”œâ”€â”€ batch_data/ (batch_order_items.csv)
â”‚       â””â”€â”€ refdata/ (reference CSVs)
â”‚
â”œâ”€â”€ bronze/
â”‚   â”œâ”€â”€ multiplex_stream                  (streamed via Spark Structured Streaming + ingestion_timestamp)
â”‚   â”œâ”€â”€ bronze_batch_order_items âš¡       (materialized via dbt model from batch_order_items.csv)
â”‚   â”œâ”€â”€ customer_segments                 (COPY INTO from refdata)
â”‚   â”œâ”€â”€ product_categories                (COPY INTO from refdata)
â”‚   â”œâ”€â”€ product_subcategories             (COPY INTO from refdata)
â”‚   â”œâ”€â”€ suppliers                         (COPY INTO from refdata)
â”‚   â”œâ”€â”€ geography âš¡                       (dbt seed â†’ materialized in Bronze)
â”‚   â””â”€â”€ (checkpoints for streaming maintained under Volumes/checkpoints/retailplex_stream)
âš¡ = Indicates dbt involvement
```
ğŸ¯ Purpose of Bronze Layer

- Bronze Layer: Store structured raw data with ingestion metadata, while preserving fidelity for downstream transformation.
- Prepares a solid foundation for the Silver layer, where cleaning, deduplication, and standardization will take place.

## ğŸ¥ˆ Silver Layer

The **Silver schema** is the **refined layer** in the Medallion Architecture.  
Here, data from Bronze is **cleaned, structured, and modeled** for business use cases.  
Different strategies are applied depending on the type of data (SCD1, SCD2, CDC, or dbt transformations).

---

### ğŸ”¹ Transformation Strategy

- **Multiplex Stream Data**  
  Split into dedicated domain tables:
  - **Customers (SCD1)**  
    - Uses Slowly Changing Dimension Type 1.  
    - Updates overwrite existing records (no history tracked).  
  - **Orders (SCD2, PySpark)**  
    - Maintains full history of changes.  
    - Implemented in **PySpark** for flexibility.  
  - **Products (SCD2, SQL)**  
    - Tracks product history over time.  
    - Implemented in **SQL** for simplicity.  
  - **Events (CDC Strategy)**  
    - A `event_cdc` table stores all insert/update/delete changes (full history).  
    - A `event` table stores only the latest snapshot of events.

- **Broadcast Reference Tables**  
  - Views are created in Silver schema from Bronze tables (`customer_segments`, `product_categories`, `product_subcategories`, `suppliers`, `geography`).  
  - Explicit schemas are defined for consistency.

- **Batch & Seed Data (via dbt âš¡)**  
  - **Batch Table (`batch_order_items`)**:  
    - Re-materialized via **dbt** in Silver schema with proper schema definitions.  
  - **Geography Seed Table**:  
    - Materialized as a **view** in Silver schema via **dbt**.

---

### ğŸ“‚ Silver Layer Flow

```plaintext
retailplex_platform/
â”œâ”€â”€ silver/
â”‚   â”œâ”€â”€ customer (SCD1, from multiplex_stream)
â”‚   â”œâ”€â”€ order (SCD2, PySpark, from multiplex_stream)
â”‚   â”œâ”€â”€ product (SCD2, SQL, from multiplex_stream)
â”‚   â”œâ”€â”€ event_cdc (full CDC history from multiplex_stream)
â”‚   â”œâ”€â”€ event (latest snapshot view from event_cdc)
â”‚   â”œâ”€â”€ broadcast_customer_segments (view from bronze)
â”‚   â”œâ”€â”€ broadcast_product_categories (view from bronze)
â”‚   â”œâ”€â”€ broadcast_product_subcategories (view from bronze)
â”‚   â”œâ”€â”€ broadcast_suppliers (view from bronze)
â”‚   â”œâ”€â”€ silver_batch_order_items âš¡ (dbt model)
â”‚   â””â”€â”€ silver_broadcast_geography âš¡ (dbt seed view)
âš¡ = Indicates dbt involvement
```
ğŸ¯ Purpose of Silver Layer

- Normalize and split multiplex data into domain tables.
- Apply SCD1, SCD2, and CDC strategies to preserve meaning and maintain history where necessary.
- Use dbt for batch and seed transformations, ensuring CI/CD integration.
- Expose clean, consistent, and business-ready datasets for downstream analytics and the Gold layer.

## ğŸ¥‡ Gold Layer â€“ Data Modelling

In the **Gold layer**, we apply **data modelling** to transform refined Silver data into **dimensions** and **fact tables**.  
These serve as the foundation for KPIs, analytics, and future data marts (FLT).  

Currently, the following **dimension (`dim_`)** and **fact (`fct_`)** tables have been materialized in **dbt âš¡**.  
FLT (data marts) will be introduced later.

---

### ğŸ“Š Dimensions (DIM)

#### 1. `gold_dim_customer`
**Purpose:** Enrich the `customer` table with attributes for segmentation and geography.

**Key Features:**
- Adds customer **segment metadata** (discount %, support level, free shipping threshold).  
- Links customers to **region & country** via geography lookup.  
- Computes **customer tenure days** from registration date.  
- Only active customers (`active_ind = 1`) are included.  

**Business Question Answered:**  
â¡ï¸ *â€œWho are our customers, what segments do they belong to, where are they located, and how long have they been with us?â€*

---

#### 2. `gold_dim_product`
**Purpose:** Create a fully enriched **product dimension**.

**Key Features:**
- Joins product with **category, subcategory, and supplier metadata**.  
- Computes **margin per unit** (`price â€“ cost`).  
- Adds supplier quality rating and region.  

**Business Question Answered:**  
â¡ï¸ *â€œWhat products do we sell, how are they categorized, who supplies them, and whatâ€™s their profit margin?â€*

---

### ğŸ“ˆ Facts (FCT)

#### 1. `gold_fct_order_items`
**Purpose:** Provide a **granular sales fact table** at the order item level.

**Key Features:**
- Includes revenue breakdowns:  
  - **Gross revenue** = qty Ã— price  
  - **Gross profit** = qty Ã— (price â€“ cost)  
  - **Net revenue** = gross â€“ discount + tax + shipping  
- Joins with product to fetch cost for margin calculations.  

**Business Question Answered:**  
â¡ï¸ *â€œHow much revenue and profit are we making at the item level?â€*

---

#### 2. `gold_fct_customer_lifetime`
**Purpose:** Aggregate a customerâ€™s **lifetime value metrics**.

**Key Features:**
- Total orders placed.  
- Lifetime revenue (gross, net) and profit.  
- **Last order date** for recency tracking.  
- **Active days span** (time between first and last order).  

**Business Question Answered:**  
â¡ï¸ *â€œWhat is each customerâ€™s lifetime value and engagement span?â€*

---

#### 3. `gold_fct_event_funnel`
**Purpose:** Track customer activity across the **conversion funnel**.

**Key Features:**
- Aggregates event data per customer/session.  
- Counts: `views`, `add_to_cart`, `checkout_started`, `purchases`.  

**Business Question Answered:**  
â¡ï¸ *â€œHow do customers progress through the funnel, and where do they drop off?â€*

### ğŸ“‚ Gold Layer Flow

```plaintext
retailplex_platform/
â”œâ”€â”€ gold/
â”‚   â”œâ”€â”€ gold_dim_customer âš¡ (dbt model)
â”‚   â”‚     - Enriched customer profile
â”‚   â”‚     - Joined with customer segments + geography
â”‚   â”‚     - Includes demographics, tenure, active filter
â”‚   â”‚
â”‚   â”œâ”€â”€ gold_dim_product âš¡ (dbt model)
â”‚   â”‚     - Enriched product attributes
â”‚   â”‚     - Categories, subcategories, suppliers
â”‚   â”‚     - Includes margin per unit (price - cost)
â”‚   â”‚
â”‚   â”œâ”€â”€ gold_fct_order_items âš¡ (dbt model)
â”‚   â”‚     - Transaction-level fact table
â”‚   â”‚     - Gross revenue, net revenue, gross profit
â”‚   â”‚
â”‚   â”œâ”€â”€ gold_fct_customer_lifetime âš¡ (dbt model)
â”‚   â”‚     - Customer-level aggregated metrics
â”‚   â”‚     - Orders, revenue, profit, last activity
â”‚   â”‚
â”‚   â””â”€â”€ gold_fct_event_funnel âš¡ (dbt model)
â”‚         - Session-level funnel
â”‚         - Views â†’ Cart Adds â†’ Checkout â†’ Purchases
âš¡ = Indicates dbt involvement
```
ğŸ¯ Purpose of the Gold Layer

- Business-friendly data â†’ Abstracts away raw/operational complexity and delivers curated, business-ready entities.
- Dimensional modeling â†’ Organizes data into Dimensions (who/what/where) and Facts (transactions/metrics) following star schema principles.
- Single source of truth â†’ Ensures consistent KPIs and metrics across reporting tools.
- Optimized for BI/Analytics â†’ Tables are pre-aggregated, joined with reference data, and designed for fast query performance.
- Governed by dbt âš¡ â†’ All models in this layer are built, materialized, and documented using dbt for transparency, lineage, and testing.
