# RetailPlex-Databrciks-dbt

This project simulates a **retail analytics platform** where data is ingested from multiple sources (streaming, batch, and reference data) and staged in **Databricks Unity Catalog** under the catalog `retailplex_platform` and schema `landing`.  

---

## ðŸ“Œ Data Sources

### 1. Streaming Multiplex Data
- Arrives as **JSONL** files under:  

/Volumes/retailplex_platform/landing/raw_files/incoming_multiplex_data/retailplex_multiplex_stream_<timestamp>.jsonl

- Each file contains multiple **topics**:
- `customers` â†’ customer profile data  
- `products` â†’ product master data  
- `orders` â†’ customer orders  
- `events` â†’ customer browsing and interaction events  

---

### 2. Reference Data (Broadcast Tables)
- Small, relatively static lookup/enrichment datasets in **CSV format**.  
- Stored under:  

/Volumes/retailplex_platform/landing/raw_files/refdata/

- Includes:
- `Customer_segments.csv`
- `Geography.csv`
- `Product_categories.csv`
- `Product_subcategories.csv`
- `Suppliers.csv`

---

### 3. Batch Data (Singleplex Full Load)
- Large transactional dataset refreshed **daily as a full load**.  
- Stored under:  

/Volumes/retailplex_platform/landing/raw_files/batch_data/batch_order_items.csv

- Loaded as managed table: landing.batch_order_items.
- Provides **order lineâ€“level transactions** linking customers, orders, products, and events.  

---
So far, the landing schema acts as the raw ingestion zone:

- Streaming JSON lines (multiplex)
- Referential CSVs (refdata)
- Bulk batch CSV (batch_order_items)

These datasets are the foundation for downstream bronze â†’ silver â†’ gold processing in the medallion architecture.

## ðŸ“‚ Data Flow till Landing Schema


```plaintext
retailplex_platform/
â”œâ”€â”€ landing/
â”‚   â”œâ”€â”€ raw_files/
â”‚   â”‚   â”œâ”€â”€ incoming_multiplex_data/
â”‚   â”‚   â”‚   â””â”€â”€ retailplex_multiplex_stream_<timestamp>.jsonl   # Streaming data
â”‚   â”‚   â”œâ”€â”€ refdata/                                            # Reference / broadcast data
â”‚   â”‚   â”‚   â”œâ”€â”€ Customer_segments.csv
â”‚   â”‚   â”‚   â”œâ”€â”€ Geography.csv
â”‚   â”‚   â”‚   â”œâ”€â”€ Product_categories.csv
â”‚   â”‚   â”‚   â”œâ”€â”€ Product_subcategories.csv
â”‚   â”‚   â”‚   â””â”€â”€ Suppliers.csv
â”‚   â”‚   â””â”€â”€ batch_data/
â”‚   â”‚       â””â”€â”€ batch_order_items.csv                           # Daily full load
â”‚   â””â”€â”€ tables/
â”‚       â””â”€â”€ batch_order_items                                   # Landing managed table
```
---

## âœ… Key Understanding
- **Multiplex streaming JSONL** â†’ multiple topics intermixed, need to be split into separate raw Bronze tables.  
- **Broadcast/reference CSVs** â†’ lookup dimensions for enrichment and joins.  
- **Batch CSV (order items)** â†’ fact-level transactional data, refreshed daily.  
- Together, these represent the **raw landing zone** of the Medallion Architecture before transformation into Bronze â†’ Silver â†’ Gold layers.  

## ðŸ¥‰ Bronze Layer (Medallion Architecture)

In the **Bronze schema**, data from the Landing layer is ingested into structured tables while still retaining raw characteristics.  
The key principle here is to **preserve the original data** but add minimal metadata such as ingestion timestamps.

### ðŸ”¹ Data Ingestion Strategy

- **Streaming Data (Multiplex Stream)**  
  - Data from `retailplex_multiplex_stream_<timestamp>.jsonl` is ingested into the Bronze table `multiplex_stream`.  
  - Implemented using **Spark Structured Streaming** (`spark.readStream` + `spark.writeStream`).  
  - A column `_ingestion_timestamp` is added (`current_timestamp()` at ingestion).  
  - Streaming checkpoints are managed in `/Volumes/checkpoints/retailplex_stream`.

- **Batch Data (DBT âš¡)**  
  - `batch_order_items.csv` (large full load dataset) is processed via **dbt**.  
  - Materialized into the Bronze schema as `bronze_batch_order_items`.

- **Broadcast Reference Tables (CSV-based)**  
  - `Customer_segments`, `Product_categories`, `Product_subcategories`, and `Suppliers` are ingested via **COPY INTO** commands in Databricks.  
  - These are relatively small lookup/reference datasets.

- **Broadcast_Geography Data (DBT Seed âš¡)**  
  - The `broadcast_geography.csv` dataset is managed as a **dbt seed**.  
  - Materialized directly in the Bronze schema.

---

### ðŸ”¹ Bronze Layer Flow

```plaintext
retailplex_platform/
â”œâ”€â”€ landing/
â”‚   â””â”€â”€ raw_files/
â”‚       â”œâ”€â”€ incoming_multiplex_data/ (multiplex JSONL stream)
â”‚       â”œâ”€â”€ batch_data/ (batch_order_items.csv)
â”‚       â””â”€â”€ refdata/ (reference CSVs)
â”‚
â”œâ”€â”€ bronze/
â”‚   â”œâ”€â”€ multiplex_stream                  (streamed via Spark Structured Streaming + ingestion_timestamp)
â”‚   â”œâ”€â”€ bronze_batch_order_items âš¡       (materialized via dbt model from batch_order_items.csv)
â”‚   â”œâ”€â”€ customer_segments                 (COPY INTO from refdata)
â”‚   â”œâ”€â”€ product_categories                (COPY INTO from refdata)
â”‚   â”œâ”€â”€ product_subcategories             (COPY INTO from refdata)
â”‚   â”œâ”€â”€ suppliers                         (COPY INTO from refdata)
â”‚   â”œâ”€â”€ geography âš¡                       (dbt seed â†’ materialized in Bronze)
â”‚   â””â”€â”€ (checkpoints for streaming maintained under Volumes/checkpoints/retailplex_stream)
âš¡ = Indicates dbt involvement
```
ðŸŽ¯ Purpose of Bronze Layer

- Bronze Layer: Store structured raw data with ingestion metadata, while preserving fidelity for downstream transformation.
- Prepares a solid foundation for the Silver layer, where cleaning, deduplication, and standardization will take place.

## ðŸ¥ˆ Silver Layer

The **Silver schema** is the **refined layer** in the Medallion Architecture.  
Here, data from Bronze is **cleaned, structured, and modeled** for business use cases.  
Different strategies are applied depending on the type of data (SCD1, SCD2, CDC, or dbt transformations).

---

### ðŸ”¹ Transformation Strategy

- **Multiplex Stream Data**  
  Split into dedicated domain tables:
  - **Customers (SCD1)**  
    - Uses Slowly Changing Dimension Type 1.  
    - Updates overwrite existing records (no history tracked).  
  - **Orders (SCD2, PySpark)**  
    - Maintains full history of changes.  
    - Implemented in **PySpark** for flexibility.  
  - **Products (SCD2, SQL)**  
    - Tracks product history over time.  
    - Implemented in **SQL** for simplicity.  
  - **Events (CDC Strategy)**  
    - A `event_cdc` table stores all insert/update/delete changes (full history).  
    - A `event` table stores only the latest snapshot of events.

- **Broadcast Reference Tables**  
  - Views are created in Silver schema from Bronze tables (`customer_segments`, `product_categories`, `product_subcategories`, `suppliers`, `geography`).  
  - Explicit schemas are defined for consistency.

- **Batch & Seed Data (via dbt âš¡)**  
  - **Batch Table (`batch_order_items`)**:  
    - Re-materialized via **dbt** in Silver schema with proper schema definitions.  
  - **Geography Seed Table**:  
    - Materialized as a **view** in Silver schema via **dbt**.

---

### ðŸ“‚ Silver Layer Flow

```plaintext
retailplex_platform/
â”œâ”€â”€ silver/
â”‚   â”œâ”€â”€ customer (SCD1, from multiplex_stream)
â”‚   â”œâ”€â”€ order (SCD2, PySpark, from multiplex_stream)
â”‚   â”œâ”€â”€ product (SCD2, SQL, from multiplex_stream)
â”‚   â”œâ”€â”€ event_cdc (full CDC history from multiplex_stream)
â”‚   â”œâ”€â”€ event (latest snapshot view from event_cdc)
â”‚   â”œâ”€â”€ broadcast_customer_segments (view from bronze)
â”‚   â”œâ”€â”€ broadcast_product_categories (view from bronze)
â”‚   â”œâ”€â”€ broadcast_product_subcategories (view from bronze)
â”‚   â”œâ”€â”€ broadcast_suppliers (view from bronze)
â”‚   â”œâ”€â”€ silver_batch_order_items âš¡ (dbt model)
â”‚   â””â”€â”€ silver_broadcast_geography âš¡ (dbt seed view)
âš¡ = Indicates dbt involvement
```
ðŸŽ¯ Purpose of Silver Layer

- Normalize and split multiplex data into domain tables.
- Apply SCD1, SCD2, and CDC strategies to preserve meaning and maintain history where necessary.
- Use dbt for batch and seed transformations, ensuring CI/CD integration.
- Expose clean, consistent, and business-ready datasets for downstream analytics and the Gold layer.
